{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37ad80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT THE RESNET\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "import blocks\n",
    "import SimpleITK as sitk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2655b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescale_limits = (0,0.75)\n",
    "max(rescale_limits)\n",
    "min(rescale_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0150569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 22, 256, 256])\n",
      "torch.Size([2, 23, 1024, 1024])\n",
      "torch.Size([2, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# TESTING PIX2PIX\n",
    "\n",
    "a = torch.randn(2,1,512,512)\n",
    "b = torch.randn(2,1,1024,1024)\n",
    "\n",
    "m = nn.ConvTranspose2d(1, 22, kernel_size=3, stride=2)\n",
    "enc = blocks.Pix2Pix_Encoder_Block(1,22, _normType=None)\n",
    "print(enc(a).shape)\n",
    "dec = blocks.Pix2Pix_DecoderBlock( _in_channels=1, _out_channels=22, _kernel_size=(4,4), _stride=(2,2), _padding=(1,1), _dilation=(1,1), _normType=\"BatchNorm\", _dropoutType=None)\n",
    "print(dec(a,b).shape)\n",
    "\n",
    "# Generator\n",
    "generator = blocks.Generator_Pix2Pix(a.shape)\n",
    "print(generator(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9acb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]])\n",
      "torch.Size([2, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# TESTING RESUNET (Zhang 2018)\n",
    "def weights_init(m):\n",
    "    # From DCGAN paper\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        if m.affine:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    for i in m.children():\n",
    "        # Specific weight setting for ResUNet shortcut.\n",
    "        if i.__class__.__name__ == \"ResUNet_shortcut\":\n",
    "            for ii in i.children():\n",
    "                if isinstance(ii, nn.Conv2d) or isinstance(ii, nn.ConvTranspose2d):\n",
    "                    nn.init.constant_(ii.weight.data, 1.)\n",
    "            for param in i.parameters():\n",
    "                param.requires_grad=False\n",
    "                \n",
    "\n",
    "a = torch.randn(2,1,512,512)\n",
    "gen = blocks.Generator_ResUNet(input_array_shape=a.shape, _first_out_channels=64, _reluType=\"leaky\")\n",
    "gen.apply(weights_init)\n",
    "print(gen(a).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4891ae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 256, 256])\n",
      "torch.Size([1, 20, 256, 256])\n",
      "torch.Size([1, 16, 256, 256])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 2048, 8, 8])\n",
      "torch.Size([1, 1])\n",
      "TESTING BRIDGE\n",
      "torch.Size([1, 4, 64, 64])\n",
      "torch.Size([1, 16, 256, 256])\n",
      "torch.Size([1, 512, 16, 16])\n",
      "torch.Size([1, 16, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nfdlam\\.conda\\envs\\AI\\lib\\site-packages\\torch\\nn\\functional.py:3455: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    }
   ],
   "source": [
    "# TESTING RESUNET-A COMPONENTS\n",
    "\n",
    "m = blocks.ResUNet_A_miniBlock(16)\n",
    "n = blocks.Conv2DN(16,20)\n",
    "p = blocks.ResUNet_A_Block_4(16, _kernel_size=(3,3), _dilation_rates=[1,3,5,7])\n",
    "ds = blocks.DownSample(16)\n",
    "mp = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "\n",
    "a = torch.randn(1, 16, 256, 256)\n",
    "c = torch.randn(1, 2, 512, 512)\n",
    "d = torch.randn(1, 1024, 8, 8)\n",
    "\n",
    "enc = blocks.Encoder_ResUNet_A_d7(32, c.shape)\n",
    "msc = blocks.MultiScale_Classifier(_input_channels=32, _input_array_shape=c.shape)\n",
    "\n",
    "print(m(a).shape)\n",
    "print(n(a).shape)\n",
    "print(p(a).shape)\n",
    "print(ds(a).shape)\n",
    "print(enc(c).shape)\n",
    "print(msc(c).shape)\n",
    "\n",
    "print(\"TESTING BRIDGE\")\n",
    "\n",
    "output_size = (a.shape[2]//4,a.shape[3]//4)\n",
    "mp = blocks.PSPPooling_miniBlock(_in_channels=16, _output_size=output_size, _kernel_size=output_size, _stride=output_size, _padding=0, _dilation=(1,1), _pyramid_levels=4)\n",
    "print(mp(a).shape)\n",
    "\n",
    "pspp = blocks.PSPPooling(_tensor_array_shape = a.shape)\n",
    "print(pspp(a).shape)\n",
    "\n",
    "\n",
    "upsh = blocks.UpSampleAndHalveChannels( d.shape[1])\n",
    "print(upsh(d).shape)\n",
    "\n",
    "t1 = torch.randn(1,32,128,128)\n",
    "upsh = blocks.UpSampleAndHalveChannels( t1.shape[1])\n",
    "cbn= blocks.Combine(_in_channels=16)\n",
    "print(cbn(a,upsh(t1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f55f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nfdlam\\.conda\\envs\\AI\\lib\\site-packages\\torch\\nn\\functional.py:3455: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Generator\n",
    "print(tuple(map(lambda x: int(x), (1., 2.))))\n",
    "\n",
    "input_test = torch.randn(1,2,512, 512)\n",
    "generator = blocks.Generator_ResUNet_A(_input_channels=16, _input_array_shape=input_test.shape, _norm_type='BatchNorm', _ADL_drop_rate=0.75, _ADL_gamma=0.9)\n",
    "\n",
    "print(generator(input_test).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
