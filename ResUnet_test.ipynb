{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92614b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT THE RESNET\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "import blocks, transforms, losses, ZhouModel, pix2pix\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import transforms, blocks, dataloaders, pix2pix\n",
    "import os, sys, pathlib, math\n",
    "import random\n",
    "from skimage import data\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as tvtransforms\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DRR_directory = \"D:\\data\\CT-Covid-19-August2020\\DRR_output\"\n",
    "discriminator_keys_images = [\"source\",\"boneless\",\"lung\"]\n",
    "image_spatial_size = (256,256)\n",
    "ds_discriminator = dataloaders.Dataset_CTCovid19August2020(os.path.join(DRR_directory,\"DRR_directory.csv\"), \n",
    "                                               os.path.join(DRR_directory), \n",
    "                                 transform=tvtransforms.Compose([\n",
    "                                     transforms.Rescale(image_spatial_size, discriminator_keys_images, \"PixelSize\"),\n",
    "                                     transforms.NormaliseBetweenPlusMinus1(discriminator_keys_images),\n",
    "                                     transforms.IntensityJitter(discriminator_keys_images),\n",
    "                                     transforms.ToTensor(discriminator_keys_images),\n",
    "                                     ]))\n",
    "dl = DataLoader(ds_discriminator, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b16ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "0\n",
      "4\n",
      "8\n",
      "torch.Size([4])\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (6 x 6). Kernel size: (13 x 13). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4c616f7a7c79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mimage_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZhouModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_fake\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nfdlam\\.conda\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nfdlam\\Desktop\\CXR-GAN\\ZhouModel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mout1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mout2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mout3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0mout4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdconv4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mout5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdconv5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nfdlam\\.conda\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nfdlam\\Desktop\\CXR-GAN\\ZhouModel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilatedConv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nfdlam\\.conda\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nfdlam\\.conda\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nfdlam\\.conda\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 396\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (6 x 6). Kernel size: (13 x 13). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4,1,16,16)\n",
    "b = torch.zeros(4,1,16,16)\n",
    "\n",
    "out = losses.criterion_TotalVariation(a)\n",
    "print(out.shape)\n",
    "\n",
    "pool = blocks.ImageBuffer(10)\n",
    "print(len(pool.images))\n",
    "out = pool.query(a)\n",
    "print(len(pool.images))\n",
    "out = pool.query(b)\n",
    "print(len(pool.images))\n",
    "\n",
    "out = losses.criterion_StyleReconstruction_layer(a,b, \"mean\")\n",
    "print(out.shape)\n",
    "\n",
    "image_fake = torch.rand(4,1,256,256)\n",
    "blk = ZhouModel.Generator(image_fake.shape,)\n",
    "print(blk(a).shape)\n",
    "\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71450494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(sample[\"source\"][0,0,:])\n",
    "ax[1].imshow(sample[\"boneless\"][0,0,:])\n",
    "\n",
    "disc = blocks.Discriminator_Pix2Pix(_input_array_size=sample[\"source\"].shape)\n",
    "source = disc(sample[\"source\"])\n",
    "boneless = disc(sample[\"boneless\"])\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(source.detach()[0,0,:,:])\n",
    "ax[1].imshow(boneless.detach()[0,0,:,:])\n",
    "\n",
    "bceWLL = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(bceWLL(source, torch.ones(source.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac14e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/models/base_model.py#L219\n",
    "    Set requires_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "    Parameters:\n",
    "        nets (network list)   -- a list of networks\n",
    "        requires_grad (bool)  -- whether the networks require gradients or not\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "    return net\n",
    "\n",
    "test = torch.randn(3)\n",
    "model = nn.Sequential(nn.Linear(3,10),nn.Linear(10,10),nn.Linear(10,1))\n",
    "\n",
    "model = set_requires_grad(model, False)\n",
    "    \n",
    "model = set_requires_grad(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING PIX2PIX\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\"\"\"a = torch.randn(2,1,512,512)\n",
    "b = torch.randn(2,1,1024,1024)\n",
    "\n",
    "m = nn.ConvTranspose2d(1, 22, kernel_size=3, stride=2)\n",
    "enc = blocks.Pix2Pix_Encoder_Block(1,22, _normType=None)\n",
    "#print(enc(a).shape)\n",
    "dec = blocks.Pix2Pix_DecoderBlock( _in_channels=1, _out_channels=22, _kernel_size=(4,4), _stride=(2,2), _padding=(1,1), _dilation=(1,1), _normType=\"BatchNorm\", _dropoutType=None)\n",
    "#print(dec(a,b).shape)\n",
    "\"\"\"\n",
    "# Generator\n",
    "a = torch.randn(2,1,512,512)\n",
    "generator = pix2pix.Generator_Pix2Pix(a.shape)\n",
    "gen_out = generator(a)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "b = torch.randn(2,2,512,512)\n",
    "disc = pix2pix.Discriminator_Pix2Pix(_input_array_size=b.shape, _first_out_channels=64, _normType=\"BatchNorm\", spectral_normalize=False)\n",
    "out = disc(b)\n",
    "\n",
    "make_dot(gen_out.mean(), params=dict(generator.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING RESUNET (Zhang 2018)\n",
    "def weights_init(m):\n",
    "    # From DCGAN paper\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        if m.affine:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    for i in m.children():\n",
    "        # Specific weight setting for ResUNet shortcut.\n",
    "        if i.__class__.__name__ == \"ResUNet_shortcut\":\n",
    "            for ii in i.children():\n",
    "                if isinstance(ii, nn.Conv2d) or isinstance(ii, nn.ConvTranspose2d):\n",
    "                    nn.init.constant_(ii.weight.data, 1.)\n",
    "            for param in i.parameters():\n",
    "                param.requires_grad=False\n",
    "                \n",
    "\n",
    "a = torch.randn(2,1,512,512)\n",
    "gen = blocks.Generator_ResUNet(input_array_shape=a.shape, _first_out_channels=64, _reluType=\"leaky\")\n",
    "gen.apply(weights_init)\n",
    "print(gen(a).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,1,512,512)\n",
    "gen2 = blocks.Generator_ResUNet_PixelShuffle(input_array_shape=a.shape, _first_out_channels=64, _reluType=\"leaky\")\n",
    "out = gen2(a)\n",
    "print(out[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a61b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING RESUNET-A COMPONENTS\n",
    "\n",
    "m = blocks.ResUNet_A_miniBlock(16)\n",
    "n = blocks.Conv2DN(16,20)\n",
    "p = blocks.ResUNet_A_Block_4(16, _kernel_size=(3,3), _dilation_rates=[1,3,5,7])\n",
    "ds = blocks.DownSample(16)\n",
    "mp = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "\n",
    "a = torch.randn(1, 16, 256, 256)\n",
    "c = torch.randn(1, 2, 512, 512)\n",
    "d = torch.randn(1, 1024, 8, 8)\n",
    "\n",
    "enc = blocks.Encoder_ResUNet_A_d7(32, c.shape)\n",
    "msc = blocks.MultiScale_Classifier(_input_channels=32, _input_array_shape=c.shape)\n",
    "\n",
    "print(m(a).shape)\n",
    "print(n(a).shape)\n",
    "print(p(a).shape)\n",
    "print(ds(a).shape)\n",
    "print(enc(c).shape)\n",
    "print(msc(c).shape)\n",
    "\n",
    "print(\"TESTING BRIDGE\")\n",
    "\n",
    "output_size = (a.shape[2]//4,a.shape[3]//4)\n",
    "mp = blocks.PSPPooling_miniBlock(_in_channels=16, _output_size=output_size, _kernel_size=output_size, _stride=output_size, _padding=0, _dilation=(1,1), _pyramid_levels=4)\n",
    "print(mp(a).shape)\n",
    "\n",
    "pspp = blocks.PSPPooling(_tensor_array_shape = a.shape)\n",
    "print(pspp(a).shape)\n",
    "\n",
    "\n",
    "upsh = blocks.UpSampleAndHalveChannels( d.shape[1])\n",
    "print(upsh(d).shape)\n",
    "\n",
    "t1 = torch.randn(1,32,128,128)\n",
    "upsh = blocks.UpSampleAndHalveChannels( t1.shape[1])\n",
    "cbn= blocks.Combine(_in_channels=16)\n",
    "print(cbn(a,upsh(t1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a567792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "print(tuple(map(lambda x: int(x), (1., 2.))))\n",
    "\n",
    "input_test = torch.randn(1,2,512, 512)\n",
    "generator = blocks.Generator_ResUNet_A(_input_channels=16, _input_array_shape=input_test.shape, _norm_type='BatchNorm', _ADL_drop_rate=0.75, _ADL_gamma=0.9)\n",
    "\n",
    "print(generator(input_test).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
