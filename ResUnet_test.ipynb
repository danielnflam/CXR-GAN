{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbc63c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-982c33ada857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOhModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDeconvResBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "# IMPLEMENT THE RESNET\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import os, sys, pathlib, math\n",
    "import random\n",
    "from skimage import data\n",
    "import pywt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as tvtransforms\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchviz import make_dot\n",
    "import blocks, transforms, losses, ZhouModel, pix2pix, dataloaders, OhModel\n",
    "\n",
    "data_BSE = os.path.normpath(\"D:/data/JSRT/augmented/augmented/target\")\n",
    "data_normal = os.path.normpath(\"D:/data/JSRT/augmented/augmented/source\")\n",
    "discriminator_keys_images = [\"source\",\"boneless\"]\n",
    "image_spatial_size = (256,256)\n",
    "ds_discriminator = dataloaders.JSRT_CXR(data_normal, data_BSE, \n",
    "                                        transform=tvtransforms.Compose([\n",
    "                             transforms.RescalingNormalisation(discriminator_keys_images,(0,1)),\n",
    "                             transforms.Rescale(image_spatial_size, discriminator_keys_images, None),\n",
    "                             transforms.HaarTransform(discriminator_keys_images),\n",
    "                             transforms.ToTensor(discriminator_keys_images),\n",
    "                             ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d08d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image SHAPE is: torch.Size([3, 4, 128, 128])\n",
      "Minibatch Discrimination\n",
      "torch.Size([3, 65586])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 4, 4])\n",
      "tensor([0., 0., 0.])\n",
      "0\n",
      "3\n",
      "6\n",
      "Testing UpsampleAndConv\n",
      "Image SHAPE is: torch.Size([3, 4, 128, 128])\n",
      "torch.Size([3, 16, 256, 256])\n",
      "Oh Model\n",
      "torch.Size([3, 64, 64, 64])\n",
      "torch.Size([3, 64, 256, 256])\n",
      "torch.Size([3, 16, 128, 128])\n",
      "Oh Model Generator thought to use summation skip connection.\n",
      "Oh Model Discriminator\n",
      "tensor([[-0.3090],\n",
      "        [-0.0090],\n",
      "        [ 0.0018]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# DL\n",
    "dl = DataLoader(ds_discriminator, batch_size=3, shuffle=True, num_workers=0)\n",
    "sample = next(iter(dl))\n",
    "image = sample[\"source\"]\n",
    "print(\"Image SHAPE is: \" +str(image.shape))\n",
    "\n",
    "# Minibatch Discrimination\n",
    "mbd = blocks.MiniBatchDiscrimination(image.flatten(start_dim=1).shape[1], 50, 50)\n",
    "print(\"Minibatch Discrimination\")\n",
    "print(mbd(image.flatten(start_dim=1)).shape)\n",
    "\n",
    "# Losses\n",
    "out = losses.criterion_TotalVariation(image)\n",
    "print(out.shape)\n",
    "out = losses.criterion_StyleReconstruction_layer(image, image, \"mean\")\n",
    "print(out)\n",
    "\n",
    "# Image Buffer\n",
    "pool = blocks.ImageBuffer(10)\n",
    "print(len(pool.images))\n",
    "out = pool.query(image)\n",
    "print(len(pool.images))\n",
    "out = pool.query(image)\n",
    "print(len(pool.images))\n",
    "\n",
    "print(\"Testing UpsampleAndConv\")\n",
    "print(\"Image SHAPE is: \" +str(image.shape))\n",
    "blk = blocks.UpsampleConvolution(4,16,upsample_scale_factor=2,kernel_size=1)\n",
    "print(blk(image).shape)\n",
    "\n",
    "print(\"Oh Model\")\n",
    "blkC = OhModel.ConvResBlock(4,64)\n",
    "print(blkC(image).shape)\n",
    "blk = OhModel.DeconvResBlock(4,64)\n",
    "print(blk(image).shape)\n",
    "blkSE = OhModel.SqueezeExcitationBlock(in_c=4, out_c=16)\n",
    "print(blkSE(image).shape)\n",
    "Generator = OhModel.Generator(input_array_shape=image.shape)\n",
    "#make_dot(Generator(image).mean(), params=dict(Generator.named_parameters()))\n",
    "\n",
    "Disc = OhModel.Discriminator(image.shape, 50, 50)\n",
    "print(\"Oh Model Discriminator\")\n",
    "print(Disc(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,3,4,5)\n",
    "for b in a:\n",
    "    print(b.shape)\n",
    "print(3//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP\n",
    "\n",
    "# Test elementwise mult\n",
    "a = torch.rand(2,1,4,4)\n",
    "b = torch.zeros(2,1).unsqueeze(-1).unsqueeze(-1)\n",
    "print((a*b))\n",
    "\n",
    "# Pix2Pix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pix2pix\n",
    "from torchviz import make_dot\n",
    "a = torch.rand(4,1,256,256)\n",
    "gen = pix2pix.UnetGenerator(input_nc=1, output_nc=1, num_downs=8, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=True)\n",
    "gen_out = gen(a)\n",
    "make_dot(gen_out.mean(), params=dict(gen.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pix2Pix\n",
    "import torch\n",
    "import pix2pix\n",
    "from torchviz import make_dot\n",
    "\n",
    "a = torch.rand(4,1,256,256)\n",
    "gen = pix2pix.Custom_Written_Generator(a.shape)\n",
    "gen_out = gen(a)\n",
    "make_dot(gen_out.mean(), params=dict(gen.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daca23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/models/base_model.py#L219\n",
    "    Set requires_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "    Parameters:\n",
    "        nets (network list)   -- a list of networks\n",
    "        requires_grad (bool)  -- whether the networks require gradients or not\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "    return net\n",
    "\n",
    "test = torch.randn(3)\n",
    "model = nn.Sequential(nn.Linear(3,10),nn.Linear(10,10),nn.Linear(10,1))\n",
    "\n",
    "model = set_requires_grad(model, False)\n",
    "    \n",
    "model = set_requires_grad(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING PIX2PIX\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\"\"\"a = torch.randn(2,1,512,512)\n",
    "b = torch.randn(2,1,1024,1024)\n",
    "\n",
    "m = nn.ConvTranspose2d(1, 22, kernel_size=3, stride=2)\n",
    "enc = blocks.Pix2Pix_Encoder_Block(1,22, _normType=None)\n",
    "#print(enc(a).shape)\n",
    "dec = blocks.Pix2Pix_DecoderBlock( _in_channels=1, _out_channels=22, _kernel_size=(4,4), _stride=(2,2), _padding=(1,1), _dilation=(1,1), _normType=\"BatchNorm\", _dropoutType=None)\n",
    "#print(dec(a,b).shape)\n",
    "\"\"\"\n",
    "# Generator\n",
    "a = torch.randn(2,1,512,512)\n",
    "generator = pix2pix.Generator_Pix2Pix(a.shape)\n",
    "gen_out = generator(a)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "b = torch.randn(2,2,512,512)\n",
    "disc = pix2pix.Discriminator_Pix2Pix(_input_array_size=b.shape, _first_out_channels=64, _normType=\"BatchNorm\", spectral_normalize=False)\n",
    "out = disc(b)\n",
    "\n",
    "make_dot(gen_out.mean(), params=dict(generator.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56573554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING RESUNET (Zhang 2018)\n",
    "def weights_init(m):\n",
    "    # From DCGAN paper\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        if m.affine:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    for i in m.children():\n",
    "        # Specific weight setting for ResUNet shortcut.\n",
    "        if i.__class__.__name__ == \"ResUNet_shortcut\":\n",
    "            for ii in i.children():\n",
    "                if isinstance(ii, nn.Conv2d) or isinstance(ii, nn.ConvTranspose2d):\n",
    "                    nn.init.constant_(ii.weight.data, 1.)\n",
    "            for param in i.parameters():\n",
    "                param.requires_grad=False\n",
    "                \n",
    "\n",
    "a = torch.randn(2,1,512,512)\n",
    "gen = blocks.Generator_ResUNet(input_array_shape=a.shape, _first_out_channels=64, _reluType=\"leaky\")\n",
    "gen.apply(weights_init)\n",
    "print(gen(a).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53189f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,1,512,512)\n",
    "gen2 = blocks.Generator_ResUNet_PixelShuffle(input_array_shape=a.shape, _first_out_channels=64, _reluType=\"leaky\")\n",
    "out = gen2(a)\n",
    "print(out[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38195d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING RESUNET-A COMPONENTS\n",
    "\n",
    "m = blocks.ResUNet_A_miniBlock(16)\n",
    "n = blocks.Conv2DN(16,20)\n",
    "p = blocks.ResUNet_A_Block_4(16, _kernel_size=(3,3), _dilation_rates=[1,3,5,7])\n",
    "ds = blocks.DownSample(16)\n",
    "mp = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "\n",
    "a = torch.randn(1, 16, 256, 256)\n",
    "c = torch.randn(1, 2, 512, 512)\n",
    "d = torch.randn(1, 1024, 8, 8)\n",
    "\n",
    "enc = blocks.Encoder_ResUNet_A_d7(32, c.shape)\n",
    "msc = blocks.MultiScale_Classifier(_input_channels=32, _input_array_shape=c.shape)\n",
    "\n",
    "print(m(a).shape)\n",
    "print(n(a).shape)\n",
    "print(p(a).shape)\n",
    "print(ds(a).shape)\n",
    "print(enc(c).shape)\n",
    "print(msc(c).shape)\n",
    "\n",
    "print(\"TESTING BRIDGE\")\n",
    "\n",
    "output_size = (a.shape[2]//4,a.shape[3]//4)\n",
    "mp = blocks.PSPPooling_miniBlock(_in_channels=16, _output_size=output_size, _kernel_size=output_size, _stride=output_size, _padding=0, _dilation=(1,1), _pyramid_levels=4)\n",
    "print(mp(a).shape)\n",
    "\n",
    "pspp = blocks.PSPPooling(_tensor_array_shape = a.shape)\n",
    "print(pspp(a).shape)\n",
    "\n",
    "\n",
    "upsh = blocks.UpSampleAndHalveChannels( d.shape[1])\n",
    "print(upsh(d).shape)\n",
    "\n",
    "t1 = torch.randn(1,32,128,128)\n",
    "upsh = blocks.UpSampleAndHalveChannels( t1.shape[1])\n",
    "cbn= blocks.Combine(_in_channels=16)\n",
    "print(cbn(a,upsh(t1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563eb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "print(tuple(map(lambda x: int(x), (1., 2.))))\n",
    "\n",
    "input_test = torch.randn(1,2,512, 512)\n",
    "generator = blocks.Generator_ResUNet_A(_input_channels=16, _input_array_shape=input_test.shape, _norm_type='BatchNorm', _ADL_drop_rate=0.75, _ADL_gamma=0.9)\n",
    "\n",
    "print(generator(input_test).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
